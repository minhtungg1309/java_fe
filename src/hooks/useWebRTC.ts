import { useRef, useState, useCallback, useMemo } from 'react';
import { CallOffer, CallAnswer, IceCandidate } from './useSocket';

export interface WebRTCState {
  localStream: MediaStream | null;
  remoteStream: MediaStream | null;
  isCallActive: boolean;
  isVideoEnabled: boolean;
  isAudioEnabled: boolean;
  callType: 'audio' | 'video';
  error: string | null;
  deviceStatus: {
    hasAudio: boolean;
    hasVideo: boolean;
    permissionGranted: boolean;
  };
  isNoDeviceMode: boolean;
  canProceedWithoutDevices: boolean;
}

export function useWebRTC() {
  const [state, setState] = useState<WebRTCState>({
    localStream: null,
    remoteStream: null,
    isCallActive: false,
    isVideoEnabled: false,
    isAudioEnabled: false,
    callType: 'audio',
    error: null,
    deviceStatus: {
      hasAudio: false,
      hasVideo: false,
      permissionGranted: false,
    },
    isNoDeviceMode: false,
    canProceedWithoutDevices: true,
  });

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const localVideoRef = useRef<HTMLVideoElement>(null!);
  const remoteVideoRef = useRef<HTMLVideoElement>(null!);
  
  // TH√äM: B·ªô ƒë·ªám ICE candidate
  const iceCandidateBufferRef = useRef<IceCandidate[]>([]);
  const remoteDescriptionSetRef = useRef<boolean>(false);
  const isCallerRef = useRef<boolean>(false);

  // C·∫•u h√¨nh m√°y ch·ªß ICE
  const iceServers = useMemo(() => ({
    iceServers: [
      { urls: 'stun:stun.l.google.com:19302' },
      { urls: 'stun:stun1.l.google.com:19302' },
      { urls: 'stun:stun2.l.google.com:19302' },
      { urls: 'stun:stun3.l.google.com:19302' },
    ],
  }), []);

  /**
   * T·∫°o track √¢m thanh im l·∫∑ng cho cu·ªôc g·ªçi kh√¥ng c√≥ microphone
   */
  const createSilentAudioTrack = useCallback(() => {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    const oscillator = audioContext.createOscillator();
    const gainNode = audioContext.createGain();
    
    oscillator.connect(gainNode);
    gainNode.connect(audioContext.destination);
    gainNode.gain.value = 0; // Im l·∫∑ng
    oscillator.frequency.value = 440;
    oscillator.start();
    
    // T·∫°o MediaStream t·ª´ audio context
    const destination = audioContext.createMediaStreamDestination();
    gainNode.connect(destination);
    
    return destination.stream.getAudioTracks()[0];
  }, []);

  /**
   * T·∫°o track video ƒëen cho cu·ªôc g·ªçi kh√¥ng c√≥ camera
   */
  const createBlackVideoTrack = useCallback(() => {
    const canvas = document.createElement('canvas');
    canvas.width = 640;
    canvas.height = 480;
    
    const ctx = canvas.getContext('2d')!;
    ctx.fillStyle = 'black';
    ctx.fillRect(0, 0, canvas.width, canvas.height);
    
    // Th√™m text overlay
    ctx.fillStyle = 'white';
    ctx.font = '24px Arial';
    ctx.textAlign = 'center';
    ctx.fillText('Kh√¥ng c√≥ Camera', canvas.width / 2, canvas.height / 2);
    
    const stream = canvas.captureStream(15); // 15fps
    return stream.getVideoTracks()[0];
  }, []);

  /**
   * Ki·ªÉm tra h·ªó tr·ª£ WebRTC
   */
  const checkWebRTCSupport = useCallback(() => {
    const isLocalhost = window.location.hostname === 'localhost' || 
                       window.location.hostname === '127.0.0.1' ||
                       window.location.hostname === '[::1]';
    
    const isHTTPS = window.location.protocol === 'https:';
    
    if (!isLocalhost && !isHTTPS) {
      const errorMessage = 'WebRTC y√™u c·∫ßu HTTPS. Vui l√≤ng truy c·∫≠p trang web qua HTTPS.';
      setState(prev => ({ ...prev, error: errorMessage }));
      return false;
    }

    const isSupported = !!(
      navigator.mediaDevices &&
      typeof navigator.mediaDevices.getUserMedia === 'function' &&
      window.RTCPeerConnection
    );

    if (!isSupported) {
      const errorMessage = 'WebRTC kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng c·∫≠p nh·∫≠t tr√¨nh duy·ªát.';
      setState(prev => ({ ...prev, error: errorMessage }));
      return false;
    }

    return true;
  }, []);

  /**
   * Ki·ªÉm tra thi·∫øt b·ªã media c√≥ s·∫µn
   */
  const checkMediaDevices = useCallback(async () => {
    try {
      if (!checkWebRTCSupport()) {
        return { hasAudio: false, hasVideo: false };
      }

      // Th·ª≠ l·∫•y danh s√°ch thi·∫øt b·ªã tr∆∞·ªõc
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioDevices = devices.filter(device => device.kind === 'audioinput');
        const videoDevices = devices.filter(device => device.kind === 'videoinput');
        
        const hasAudio = audioDevices.length > 0;
        const hasVideo = videoDevices.length > 0;
        
        console.log('üéß Thi·∫øt b·ªã c√≥ s·∫µn:', {
          audioDevices: audioDevices.length,
          videoDevices: videoDevices.length,
        });

        setState(prev => ({
          ...prev,
          deviceStatus: {
            hasAudio,
            hasVideo,
            permissionGranted: audioDevices.some(d => d.label !== '') || videoDevices.some(d => d.label !== ''),
          },
          isNoDeviceMode: !hasAudio && !hasVideo,
        }));

        return { hasAudio, hasVideo };
      } catch (enumerateError) {
        console.warn('Kh√¥ng th·ªÉ li·ªát k√™ thi·∫øt b·ªã:', enumerateError);
        
        // Fallback: th·ª≠ l·∫•y media ƒë·ªÉ ki·ªÉm tra t√≠nh kh·∫£ d·ª•ng
        try {
          const tempStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: true });
          tempStream.getTracks().forEach(track => track.stop());
          return { hasAudio: true, hasVideo: true };
        } catch (mediaError) {
          console.warn('Kh√¥ng c√≥ thi·∫øt b·ªã media n√†o kh·∫£ d·ª•ng:', mediaError);
          setState(prev => ({
            ...prev,
            deviceStatus: { hasAudio: false, hasVideo: false, permissionGranted: false },
            isNoDeviceMode: true,
          }));
          return { hasAudio: false, hasVideo: false };
        }
      }
    } catch (error) {
      console.error('Ki·ªÉm tra thi·∫øt b·ªã th·∫•t b·∫°i:', error);
      setState(prev => ({
        ...prev,
        deviceStatus: { hasAudio: false, hasVideo: false, permissionGranted: false },
        isNoDeviceMode: true,
      }));
      return { hasAudio: false, hasVideo: false };
    }
  }, [checkWebRTCSupport]);

  /**
   * Kh·ªüi t·∫°o peer connection
   */
  const initializePeerConnection = useCallback(() => {
    if (!window.RTCPeerConnection) {
      throw new Error('WebRTC kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong tr√¨nh duy·ªát n√†y');
    }

    const peerConnection = new RTCPeerConnection(iceServers);
    peerConnectionRef.current = peerConnection;

    // C·∫¢I THI·ªÜN: X·ª≠ l√Ω track t·ªët h∆°n
    peerConnection.ontrack = (event) => {
      console.log('üé• Nh·∫≠n track t·ª´ xa:', {
        kind: event.track.kind,
        enabled: event.track.enabled,
        readyState: event.track.readyState,
        streams: event.streams.length
      });
      
      const [remoteStream] = event.streams;
      if (remoteStream) {
        setState(prev => ({ ...prev, remoteStream }));
        
        if (remoteVideoRef.current) {
          remoteVideoRef.current.srcObject = remoteStream;
          // B·∫ÆT BU·ªòC play cho audio/video
          remoteVideoRef.current.play().catch(e => console.warn('Ph√°t video t·ª´ xa th·∫•t b·∫°i:', e));
        }
      }
    };

    // TH√äM: Theo d√µi tr·∫°ng th√°i k·∫øt n·ªëi
    peerConnection.onconnectionstatechange = () => {
      console.log('üîó Tr·∫°ng th√°i k·∫øt n·ªëi:', peerConnection.connectionState);
    };

    peerConnection.oniceconnectionstatechange = () => {
      console.log('üßä Tr·∫°ng th√°i k·∫øt n·ªëi ICE:', peerConnection.iceConnectionState);
    };

    return peerConnection;
  }, [iceServers]);

  /**
   * C·∫¢I THI·ªÜN: Media stream v·ªõi r√†ng bu·ªôc t·ªët h∆°n
   */
  const startMediaStream = useCallback(async (callType: 'audio' | 'video') => {
    try {
      if (!checkWebRTCSupport()) {
        throw new Error('WebRTC kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong tr√¨nh duy·ªát n√†y.');
      }

      setState(prev => ({ ...prev, error: null }));
      const { hasAudio, hasVideo } = await checkMediaDevices();
      
      console.log('üéØ T√¨nh tr·∫°ng thi·∫øt b·ªã:', { hasAudio, hasVideo, callType });

      let stream: MediaStream;

      if (hasAudio || hasVideo) {
        try {
          const constraints: MediaStreamConstraints = {
            audio: hasAudio ? {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 44100,
            } : false,
            video: (callType === 'video' && hasVideo) ? {
              width: { min: 320, ideal: 640, max: 1280 },
              height: { min: 240, ideal: 480, max: 720 },
              frameRate: { ideal: 30, max: 30 },
              facingMode: 'user'
            } : false
          };

          stream = await navigator.mediaDevices.getUserMedia(constraints);
          console.log('‚úÖ L·∫•y stream media th·∫≠t th√†nh c√¥ng:', {
            audioTracks: stream.getAudioTracks().length,
            videoTracks: stream.getVideoTracks().length,
            tracks: stream.getTracks().map(t => ({
              kind: t.kind,
              enabled: t.enabled,
              readyState: t.readyState
            }))
          });
          
        } catch (realMediaError) {
          console.warn('Media th·∫≠t th·∫•t b·∫°i, t·∫°o stream t·ªïng h·ª£p:', realMediaError);
          stream = new MediaStream();
        }
      } else {
        console.log('Kh√¥ng c√≥ thi·∫øt b·ªã n√†o kh·∫£ d·ª•ng, t·∫°o stream t·ªïng h·ª£p');
        stream = new MediaStream();
      }

      // Th√™m track t·ªïng h·ª£p n·∫øu c·∫ßn
      const audioTracks = stream.getAudioTracks();
      const videoTracks = stream.getVideoTracks();

      if (audioTracks.length === 0 && (callType === 'audio' || callType === 'video')) {
        console.log('Th√™m track √¢m thanh t·ªïng h·ª£p');
        try {
          const silentTrack = createSilentAudioTrack();
          stream.addTrack(silentTrack);
        } catch (audioError) {
          console.warn('T·∫°o track √¢m thanh im l·∫∑ng th·∫•t b·∫°i:', audioError);
        }
      }

      if (videoTracks.length === 0 && callType === 'video') {
        console.log('Th√™m track video t·ªïng h·ª£p');
        try {
          const blackTrack = createBlackVideoTrack();
          stream.addTrack(blackTrack);
        } catch (videoError) {
          console.warn('T·∫°o track video ƒëen th·∫•t b·∫°i:', videoError);
        }
      }

      const finalAudioTracks = stream.getAudioTracks();
      const finalVideoTracks = stream.getVideoTracks();
      
      console.log('üé¨ Track stream cu·ªëi c√πng:', {
        audioTracks: finalAudioTracks.length,
        videoTracks: finalVideoTracks.length,
        isNoDeviceMode: !hasAudio && !hasVideo,
        // TH√äM: Th√¥ng tin track chi ti·∫øt h∆°n
        trackDetails: stream.getTracks().map(t => ({
          kind: t.kind,
          enabled: t.enabled,
          readyState: t.readyState,
          label: t.label,
          muted: t.muted
        }))
      });

      setState(prev => ({ 
        ...prev, 
        localStream: stream, 
        callType: finalVideoTracks.length > 0 ? 'video' : 'audio',
        isVideoEnabled: finalVideoTracks.length > 0 && hasVideo,
        isAudioEnabled: finalAudioTracks.length > 0 && hasAudio,
        error: null,
        isNoDeviceMode: !hasAudio && !hasVideo,
      }));

      if (localVideoRef.current) {
        localVideoRef.current.srcObject = stream;
        localVideoRef.current.muted = true; // Tr√°nh echo
        localVideoRef.current.play().catch(e => console.warn('Ph√°t video local th·∫•t b·∫°i:', e));
      }

      return stream;

    } catch (error) {
      console.error('L·ªói trong startMediaStream:', error);
      
      try {
        console.log('T·∫°o stream t·ªïng h·ª£p kh·∫©n c·∫•p');
        const emergencyStream = new MediaStream();
        
        try {
          const silentTrack = createSilentAudioTrack();
          emergencyStream.addTrack(silentTrack);
        } catch (e) {
          console.warn('Th√™m track im l·∫∑ng v√†o stream kh·∫©n c·∫•p th·∫•t b·∫°i:', e);
        }

        setState(prev => ({ 
          ...prev, 
          localStream: emergencyStream,
          callType: 'audio',
          isVideoEnabled: false,
          isAudioEnabled: false,
          error: 'Cu·ªôc g·ªçi kh√¥ng c√≥ √¢m thanh/video (thi·∫øt b·ªã kh√¥ng kh·∫£ d·ª•ng)',
          isNoDeviceMode: true,
        }));

        return emergencyStream;
      } catch (emergencyError) {
        console.error('T·∫°o stream kh·∫©n c·∫•p th·∫•t b·∫°i:', emergencyError);
        const errorMessage = 'Kh√¥ng th·ªÉ kh·ªüi t·∫°o cu·ªôc g·ªçi. Vui l√≤ng th·ª≠ l·∫°i.';
        setState(prev => ({ ...prev, error: errorMessage }));
        throw new Error(errorMessage);
      }
    }
  }, [checkMediaDevices, createSilentAudioTrack, createBlackVideoTrack, checkWebRTCSupport]);

  /**
   * S·ª¨A: T·∫°o offer cho cu·ªôc g·ªçi video
   */
  const createOffer = useCallback(async (callType: 'audio' | 'video'): Promise<CallOffer | null> => {
    try {
      console.log('üìû T·∫°o offer cho:', callType);
      
      const peerConnection = initializePeerConnection();
      const stream = await startMediaStream(callType);

      // ƒê√°nh d·∫•u l√† ng∆∞·ªùi g·ªçi
      isCallerRef.current = true;
      remoteDescriptionSetRef.current = false;
      iceCandidateBufferRef.current = [];

      // Th√™m track ƒë√∫ng c√°ch
      stream.getTracks().forEach(track => {
        console.log('‚ûï Th√™m track v√†o peer connection:', {
          kind: track.kind,
          enabled: track.enabled,
          readyState: track.readyState
        });
        peerConnection.addTrack(track, stream);
      });

      const offer = await peerConnection.createOffer({
        offerToReceiveAudio: true,
        offerToReceiveVideo: callType === 'video'
      });
      
      await peerConnection.setLocalDescription(offer);
      console.log('‚úÖ T·∫°o offer v√† ƒë·∫∑t local description th√†nh c√¥ng');

      setState(prev => ({ ...prev, isCallActive: true }));

      return {
        callerId: '',
        calleeId: '',
        sdp: offer.sdp || '',
        type: 'offer',
        callType,
        callId: '',
      };
    } catch (error) {
      console.error('‚ùå L·ªói t·∫°o offer:', error);
      return null;
    }
  }, [initializePeerConnection, startMediaStream]);

  /**
   * S·ª¨A: T·∫°o answer v·ªõi thi·∫øt l·∫≠p ƒë√∫ng
   */
  const createAnswer = useCallback(async (offer: CallOffer): Promise<CallAnswer | null> => {
    try {
      console.log('üìû T·∫°o answer cho offer:', offer.callType);
      
      const peerConnection = initializePeerConnection();
      const stream = await startMediaStream(offer.callType);

      // ƒê√°nh d·∫•u l√† ng∆∞·ªùi nh·∫≠n
      isCallerRef.current = false;
      remoteDescriptionSetRef.current = false;
      iceCandidateBufferRef.current = [];

      // Th√™m track tr∆∞·ªõc
      stream.getTracks().forEach(track => {
        console.log('‚ûï Th√™m track v√†o peer connection (callee):', {
          kind: track.kind,
          enabled: track.enabled,
          readyState: track.readyState
        });
        peerConnection.addTrack(track, stream);
      });

      // ƒê·∫∑t remote description
      await peerConnection.setRemoteDescription({
        type: 'offer',
        sdp: offer.sdp,
      });
      
      console.log('‚úÖ ƒê·∫∑t remote description th√†nh c√¥ng (callee)');
      remoteDescriptionSetRef.current = true;

      // X·ª≠ l√Ω ICE candidate ƒë√£ buffer
      if (iceCandidateBufferRef.current.length > 0) {
        console.log(`üßä X·ª≠ l√Ω ${iceCandidateBufferRef.current.length} ICE candidate ƒë√£ buffer`);
        for (const candidate of iceCandidateBufferRef.current) {
          try {
            await peerConnection.addIceCandidate({
              candidate: candidate.candidate,
              sdpMid: candidate.sdpMid,
              sdpMLineIndex: candidate.sdpMLineIndex,
            });
          } catch (iceError) {
            console.warn('Th√™m ICE candidate ƒë√£ buffer th·∫•t b·∫°i:', iceError);
          }
        }
        iceCandidateBufferRef.current = [];
      }

      const answer = await peerConnection.createAnswer();
      await peerConnection.setLocalDescription(answer);
      console.log('‚úÖ T·∫°o answer v√† ƒë·∫∑t local description th√†nh c√¥ng');

      setState(prev => ({ ...prev, isCallActive: true }));

      return {
        callerId: offer.callerId,
        calleeId: offer.calleeId,
        sdp: answer.sdp || '',
        type: 'answer',
        callId: offer.callId,
      };
    } catch (error) {
      console.error('‚ùå L·ªói t·∫°o answer:', error);
      return null;
    }
  }, [initializePeerConnection, startMediaStream]);

  /**
   * S·ª¨A: X·ª≠ l√Ω answer v·ªõi vi·ªác x·ª≠ l√Ω ICE candidate
   */
  const handleAnswer = useCallback(async (answer: CallAnswer) => {
    try {
      console.log('üìû X·ª≠ l√Ω answer:', answer);
      
      if (peerConnectionRef.current) {
        await peerConnectionRef.current.setRemoteDescription({
          type: 'answer',
          sdp: answer.sdp,
        });
        
        console.log('‚úÖ ƒê·∫∑t remote description th√†nh c√¥ng (caller)');
        remoteDescriptionSetRef.current = true;
        
        // X·ª≠ l√Ω ICE candidate ƒë√£ buffer
        if (iceCandidateBufferRef.current.length > 0) {
          console.log(`üßä X·ª≠ l√Ω ${iceCandidateBufferRef.current.length} ICE candidate ƒë√£ buffer`);
          
          for (const candidate of iceCandidateBufferRef.current) {
            try {
              await peerConnectionRef.current.addIceCandidate({
                candidate: candidate.candidate,
                sdpMid: candidate.sdpMid,
                sdpMLineIndex: candidate.sdpMLineIndex,
              });
              console.log('‚úÖ Th√™m ICE candidate ƒë√£ buffer th√†nh c√¥ng');
            } catch (iceError) {
              console.error('‚ùå L·ªói th√™m ICE candidate ƒë√£ buffer:', iceError);
            }
          }
          
          // X√≥a buffer
          iceCandidateBufferRef.current = [];
        }
      }
    } catch (error) {
      console.error('‚ùå L·ªói x·ª≠ l√Ω answer:', error);
    }
  }, []);

  /**
   * S·ª¨A: X·ª≠ l√Ω ICE candidate v·ªõi buffer
   */
  const handleIceCandidate = useCallback(async (candidate: IceCandidate) => {
    try {
      if (!peerConnectionRef.current) {
        console.warn('‚ö†Ô∏è Kh√¥ng c√≥ peer connection cho ICE candidate');
        return;
      }

      // Buffer ICE candidate n·∫øu remote description ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t
      if (!remoteDescriptionSetRef.current) {
        console.log('üßä Buffer ICE candidate (remote description ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t)');
        iceCandidateBufferRef.current.push(candidate);
        return;
      }

      // Th√™m ICE candidate ngay l·∫≠p t·ª©c n·∫øu remote description ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t
      await peerConnectionRef.current.addIceCandidate({
        candidate: candidate.candidate,
        sdpMid: candidate.sdpMid,
        sdpMLineIndex: candidate.sdpMLineIndex,
      });
      
      console.log('‚úÖ Th√™m ICE candidate th√†nh c√¥ng');
      
    } catch (error) {
      console.error('‚ùå L·ªói x·ª≠ l√Ω ICE candidate:', error);
    }
  }, []);

  /**
   * B·∫≠t/t·∫Øt video (ch·ªâ ho·∫°t ƒë·ªông n·∫øu thi·∫øt b·ªã kh·∫£ d·ª•ng)
   */
  const toggleVideo = useCallback(() => {
    if (state.localStream && state.deviceStatus.hasVideo) {
      const videoTrack = state.localStream.getVideoTracks()[0];
      if (videoTrack) {
        videoTrack.enabled = !videoTrack.enabled;
        setState(prev => ({ ...prev, isVideoEnabled: videoTrack.enabled }));
      }
    }
  }, [state.localStream, state.deviceStatus.hasVideo]);

  /**
   * B·∫≠t/t·∫Øt audio (ch·ªâ ho·∫°t ƒë·ªông n·∫øu thi·∫øt b·ªã kh·∫£ d·ª•ng)
   */
  const toggleAudio = useCallback(() => {
    if (state.localStream && state.deviceStatus.hasAudio) {
      const audioTrack = state.localStream.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        setState(prev => ({ ...prev, isAudioEnabled: audioTrack.enabled }));
      }
    }
  }, [state.localStream, state.deviceStatus.hasAudio]);

  /**
   * S·ª¨A: K·∫øt th√∫c cu·ªôc g·ªçi v·ªõi d·ªçn d·∫πp ƒë√∫ng c√°ch
   */
  const endCall = useCallback(() => {
    if (state.localStream) {
      state.localStream.getTracks().forEach(track => track.stop());
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    // Reset t·∫•t c·∫£ refs
    iceCandidateBufferRef.current = [];
    remoteDescriptionSetRef.current = false;
    isCallerRef.current = false;

    setState({
      localStream: null,
      remoteStream: null,
      isCallActive: false,
      isVideoEnabled: false,
      isAudioEnabled: false,
      callType: 'audio',
      error: null,
      deviceStatus: {
        hasAudio: false,
        hasVideo: false,
        permissionGranted: false,
      },
      isNoDeviceMode: false,
      canProceedWithoutDevices: true,
    });

    if (localVideoRef.current) {
      localVideoRef.current.srcObject = null;
    }
    if (remoteVideoRef.current) {
      remoteVideoRef.current.srcObject = null;
    }
  }, [state.localStream]);

  const setupIceCandidateHandling = useCallback((onIceCandidate: (candidate: IceCandidate) => void, callId: string, fromUserId: string, toUserId: string) => {
    if (peerConnectionRef.current) {
      peerConnectionRef.current.onicecandidate = (event) => {
        if (event.candidate) {
          console.log('üßä ICE candidate ƒë∆∞·ª£c t·∫°o:', {
            candidate: event.candidate.candidate.substring(0, 50) + '...',
            sdpMid: event.candidate.sdpMid,
            sdpMLineIndex: event.candidate.sdpMLineIndex
          });
          
          onIceCandidate({
            candidate: event.candidate.candidate,
            sdpMid: event.candidate.sdpMid || '',
            sdpMLineIndex: event.candidate.sdpMLineIndex || 0,
            callId,
            fromUserId,
            toUserId,
          });
        }
      };
    }
  }, []);

  return {
    state,
    localVideoRef,
    remoteVideoRef,
    createOffer,
    createAnswer,
    handleAnswer,
    handleIceCandidate,
    toggleVideo,
    toggleAudio,
    endCall,
    setupIceCandidateHandling,
    checkMediaDevices,
    checkWebRTCSupport,
  };
}